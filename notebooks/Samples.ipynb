{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fba5723f-a0a6-4457-8f97-f8a7be7de7d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pykafka in /opt/conda/lib/python3.11/site-packages (2.8.0)\n",
      "Collecting confluent_kafka\n",
      "  Downloading confluent_kafka-2.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
      "Collecting avro\n",
      "  Downloading avro-1.11.3.tar.gz (90 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.6/90.6 kB\u001b[0m \u001b[31m750.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from pykafka) (1.16.0)\n",
      "Requirement already satisfied: kazoo==2.5.0 in /opt/conda/lib/python3.11/site-packages (from pykafka) (2.5.0)\n",
      "Requirement already satisfied: tabulate in /opt/conda/lib/python3.11/site-packages (from pykafka) (0.9.0)\n",
      "Downloading confluent_kafka-2.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: avro\n",
      "  Building wheel for avro (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for avro: filename=avro-1.11.3-py2.py3-none-any.whl size=123913 sha256=07504323dd27f3054c965f96e1f97c71515f5ee81e17c82c38623371438aa329\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/a8/7c/a4/fa31e47be300f6e6036f57769474de0ba54f8c6e8e2d8b7547\n",
      "Successfully built avro\n",
      "Installing collected packages: confluent_kafka, avro\n",
      "Successfully installed avro-1.11.3 confluent_kafka-2.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pykafka confluent_kafka avro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b53b15e2-67e6-4edb-ae00-1081796a7bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pykafka import KafkaClient\n",
    "\n",
    "client = KafkaClient(hosts=\"broker:29092\", broker_version=\"1.0.0\")\n",
    "\n",
    "#print(client.topics)\n",
    "\n",
    "\n",
    "topic = client.topics['training']\n",
    "\n",
    "with topic.get_sync_producer() as producer:\n",
    "    for i in range(4):\n",
    "        value = 'test message ' + str(i ** 2)\n",
    "        producer.produce(value.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a50b98-bb47-488b-baee-1ee55053b2ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01152058-5ae7-4706-b166-503711747a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ curl -fSL https://www.apache.org/dist/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz -o /tmp/hadoop.tar.gz\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   283  100   283    0     0   1693      0 --:--:-- --:--:-- --:--:--  1704\n",
      "100  696M  100  696M    0     0  7685k      0  0:01:32  0:01:32 --:--:-- 8700k0:00:42  0:01:06 8642k\n",
      "+ curl -fSL .asc -o /tmp/hadoop.tar.gz.asc\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0curl: (6) Could not resolve host: .asc\n"
     ]
    }
   ],
   "source": [
    "!set -x \\\n",
    "    && curl -fSL \"https://www.apache.org/dist/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz\" -o /tmp/hadoop.tar.gz \\\n",
    "    && curl -fSL \"$HADOOP_URL.asc\" -o /tmp/hadoop.tar.gz.asc \\\n",
    "    && tar -xvf /tmp/hadoop.tar.gz -C /opt/ \\\n",
    "    && rm /tmp/hadoop.tar.gz*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9729a8b-eec3-4ea3-ad5d-e84482b5f059",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ln -s /opt/hadoop-3.3.6/etc/hadoop /etc/hadoop \\\n",
    "    && mkdir /opt/hadoop-3.3.6/logs \\\n",
    "    && mkdir /hadoop-data \\\n",
    "    && rm -Rf /opt/hadoop-3.3.6/share/doc/hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1616c4b6-ae36-4286-a2e1-fb398725b215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 b'test message 0'\n",
      "1 b'test message 1'\n",
      "2 b'test message 4'\n",
      "3 b'test message 9'\n",
      "4 b'test message 0'\n",
      "5 b'test message 1'\n",
      "6 b'test message 4'\n",
      "7 b'test message 9'\n",
      "8 b'test message 0'\n",
      "9 b'test message 1'\n",
      "10 b'test message 4'\n",
      "11 b'test message 9'\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m consumer_topic \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mtopics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      2\u001b[0m consumer \u001b[38;5;241m=\u001b[39m consumer_topic\u001b[38;5;241m.\u001b[39mget_simple_consumer()\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m consumer:\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m message \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;28mprint\u001b[39m(message\u001b[38;5;241m.\u001b[39moffset, message\u001b[38;5;241m.\u001b[39mvalue)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pykafka/simpleconsumer.py:461\u001b[0m, in \u001b[0;36mSimpleConsumer.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Yield an infinite stream of messages until the consumer times out\"\"\"\u001b[39;00m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 461\u001b[0m     message \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconsume\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    462\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m message:\n\u001b[1;32m    463\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pykafka/simpleconsumer.py:485\u001b[0m, in \u001b[0;36mSimpleConsumer.consume\u001b[0;34m(self, block, unblock_event)\u001b[0m\n\u001b[1;32m    483\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_worker_exceptions()\n\u001b[1;32m    484\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cluster\u001b[38;5;241m.\u001b[39mhandler\u001b[38;5;241m.\u001b[39msleep()\n\u001b[0;32m--> 485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_messages_arrived\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    486\u001b[0m     \u001b[38;5;66;03m# by passing through this semaphore, we know that at\u001b[39;00m\n\u001b[1;32m    487\u001b[0m     \u001b[38;5;66;03m# least one message is waiting in some queue.\u001b[39;00m\n\u001b[1;32m    488\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_running:\n\u001b[1;32m    489\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ConsumerStoppedException()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/threading.py:479\u001b[0m, in \u001b[0;36mSemaphore.acquire\u001b[0;34m(self, blocking, timeout)\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    478\u001b[0m                 \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 479\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    481\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/threading.py:331\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 331\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    332\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    333\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "consumer_topic = client.topics['training']\n",
    "consumer = consumer_topic.get_simple_consumer()\n",
    "for message in consumer:\n",
    "    if message is not None:\n",
    "        print(message.offset, message.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "de776ce7-421a-480f-a5fd-f30687786b29",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "87",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m      3\u001b[0m topic \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mtopics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtopic_schema\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtopic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_sync_producer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproducer\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#for i in range(4):\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mname\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGökhan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mage\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m49\u001b[39;49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m        \u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Mesajı JSON formatına dönüştürün\u001b[39;49;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pykafka/producer.py:255\u001b[0m, in \u001b[0;36mProducer.__exit__\u001b[0;34m(self, exc_type, exc_value, traceback)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, exc_type, exc_value, traceback):\n\u001b[1;32m    254\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Context manager exit point - stop the producer\"\"\"\u001b[39;00m\n\u001b[0;32m--> 255\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pykafka/producer.py:331\u001b[0m, in \u001b[0;36mProducer.stop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_running:\n\u001b[1;32m    330\u001b[0m     queue_readers \u001b[38;5;241m=\u001b[39m get_queue_readers()\n\u001b[0;32m--> 331\u001b[0m     \u001b[43mstop_owned_brokers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(queue_readers) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    333\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_running \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pykafka/producer.py:324\u001b[0m, in \u001b[0;36mProducer.stop.<locals>.stop_owned_brokers\u001b[0;34m()\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstop_owned_brokers\u001b[39m():\n\u001b[0;32m--> 324\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_owned_brokers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    326\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m owned_broker \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_owned_brokers\u001b[38;5;241m.\u001b[39mvalues():\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pykafka/producer.py:567\u001b[0m, in \u001b[0;36mProducer._wait_all\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28many\u001b[39m(q\u001b[38;5;241m.\u001b[39mmessage_is_pending() \u001b[38;5;28;01mfor\u001b[39;00m q \u001b[38;5;129;01min\u001b[39;00m itervalues(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_owned_brokers)) \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    565\u001b[0m         \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_produce_has_timed_out(start_time):\n\u001b[1;32m    566\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cluster\u001b[38;5;241m.\u001b[39mhandler\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m.3\u001b[39m)\n\u001b[0;32m--> 567\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_worker_exceptions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pykafka/producer.py:240\u001b[0m, in \u001b[0;36mProducer._raise_worker_exceptions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Raises exceptions encountered on worker threads\"\"\"\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_worker_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 240\u001b[0m     \u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_worker_exception\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/six.py:718\u001b[0m, in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    716\u001b[0m         value \u001b[38;5;241m=\u001b[39m tp()\n\u001b[1;32m    717\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m value\u001b[38;5;241m.\u001b[39m__traceback__ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tb:\n\u001b[0;32m--> 718\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m value\u001b[38;5;241m.\u001b[39mwith_traceback(tb)\n\u001b[1;32m    719\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m value\n\u001b[1;32m    720\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pykafka/producer.py:621\u001b[0m, in \u001b[0;36mOwnedBroker.start.<locals>.queue_reader\u001b[0;34m()\u001b[0m\n\u001b[1;32m    619\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflush(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproducer\u001b[38;5;241m.\u001b[39m_linger_ms, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproducer\u001b[38;5;241m.\u001b[39m_max_request_size)\n\u001b[1;32m    620\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m batch:\n\u001b[0;32m--> 621\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproducer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;66;03m# surface all exceptions to the main thread\u001b[39;00m\n\u001b[1;32m    624\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproducer\u001b[38;5;241m.\u001b[39m_worker_exception \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mexc_info()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pykafka/producer.py:521\u001b[0m, in \u001b[0;36mProducer._send_request\u001b[0;34m(self, message_batch, owned_broker)\u001b[0m\n\u001b[1;32m    514\u001b[0m             info \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProduce request for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m failed with error code \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    515\u001b[0m                 topic,\n\u001b[1;32m    516\u001b[0m                 partition,\n\u001b[1;32m    517\u001b[0m                 owned_broker\u001b[38;5;241m.\u001b[39mbroker\u001b[38;5;241m.\u001b[39mhost,\n\u001b[1;32m    518\u001b[0m                 owned_broker\u001b[38;5;241m.\u001b[39mbroker\u001b[38;5;241m.\u001b[39mport,\n\u001b[1;32m    519\u001b[0m                 presponse\u001b[38;5;241m.\u001b[39merr)\n\u001b[1;32m    520\u001b[0m             log\u001b[38;5;241m.\u001b[39mwarning(info)\n\u001b[0;32m--> 521\u001b[0m             exc \u001b[38;5;241m=\u001b[39m \u001b[43mERROR_CODES\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merr\u001b[49m\u001b[43m]\u001b[49m(info)\n\u001b[1;32m    522\u001b[0m             to_retry\u001b[38;5;241m.\u001b[39mextend(\n\u001b[1;32m    523\u001b[0m                 (mset, exc)\n\u001b[1;32m    524\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m mset \u001b[38;5;129;01min\u001b[39;00m _get_partition_msgs(partition, req))\n\u001b[1;32m    525\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketDisconnectedError, struct\u001b[38;5;241m.\u001b[39merror) \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[0;31mKeyError\u001b[0m: 87"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "topic = client.topics['topic_schema']\n",
    "\n",
    "with topic.get_sync_producer() as producer:\n",
    "    #for i in range(4):\n",
    "\n",
    "    value = {\n",
    "        \"name\":\"Gökhan\",\n",
    "        \"age\":49\n",
    "    }        \n",
    "\n",
    "    # Mesajı JSON formatına dönüştürün\n",
    "    message_json = json.dumps(value)\n",
    "    \n",
    "    # Mesajı bytes'a dönüştürün\n",
    "    message_bytes = message_json.encode('utf-8')\n",
    "    #value = 'test message ' + str(i ** 2)\n",
    "    producer.produce(message_bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "385c9332-b22f-4567-8293-6d38a9f2eb9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ccdcebc0569e:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f14b4803fd0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .config(\"spark.streaming.stopGracefullyOnShutdown\", True) \\\n",
    "    .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0') \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", 4) \\\n",
    "    .master(\"local\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29cc1070-43a7-42d7-8f0a-9e430451341e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_820/377846958.py:11: DeprecationWarning: AvroProducer has been deprecated. Use AvroSerializer instead.\n",
      "  topic_producer = AvroProducer(producer_config, default_value_schema=avro.schema.parse(open(\"sample-schema.avsc\", \"rb\").read()))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import avro\n",
    "from avro import schema, datafile, io\n",
    "from confluent_kafka.avro import AvroProducer\n",
    "\n",
    "\n",
    "producer_config =  {\n",
    "                    'bootstrap.servers': 'broker:29092',\n",
    "                    'schema.registry.url': 'http://schema-registry:8081'\n",
    "                    }\n",
    "\n",
    "topic_producer = AvroProducer(producer_config, default_value_schema=avro.schema.parse(open(\"sample-schema.avsc\", \"rb\").read()))\n",
    "\n",
    "\n",
    "value = {\n",
    "        \"name\":\"Gökhan\",\n",
    "        \"age\":49\n",
    "        } \n",
    "topic_producer.produce(topic=\"topic_schema\",value=value)\n",
    "topic_producer.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f8a83912-3ff2-4f27-9325-7837a0f3a338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n",
      "root\n",
      " |-- value: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "streaming_df = spark.readStream\\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"broker:29092\") \\\n",
    "    .option(\"subscribe\", \"topic_schema\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "streaming_df.printSchema()\n",
    "\n",
    "json_df = streaming_df.selectExpr(\"cast(value as string) as value\", \"timestamp\")\n",
    "json_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8c4c5b-546f-41d5-b84d-b757ce81e1e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      "\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StringType, StructField, StructType, ArrayType, LongType\n",
    "\n",
    "json_schema = StructType(\n",
    "    [StructField('name', StringType(), True), \\\n",
    "     StructField('age', LongType(), True)])\n",
    "\n",
    "json_expanded_df = json_df.withColumn(\"value\", from_json(json_df[\"value\"], json_schema)).select(col(\"value.age\"),col(\"value.name\"),col(\"timestamp\")) \n",
    "json_expanded_df.printSchema()\n",
    "\n",
    "flattened_df = json_expanded_df.selectExpr(\"name\", \"age\",\"timestamp\")\n",
    "flattened_df.printSchema()\n",
    "#agg_df = flattened_df.agg(avg(\"age\").alias(\"age_temp\"))\n",
    "\n",
    "flattened_df.withWatermark(\"timestamp\", \"10 seconds\").writeStream \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"path\", \"output.csv\") \\\n",
    "    .option(\"append\", \"true\") \\\n",
    "    .option(\"checkpointLocation\", \"checkpoint\") \\\n",
    "    .start()\n",
    "spark.streams.awaitAnyTermination()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
